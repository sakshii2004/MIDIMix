{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f7513d-9604-4805-918a-ad7a8d5416e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de694b8e-f20f-4d6f-a70b-e7c8049813ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96c8c99-ef1d-4d66-b373-5280a8035296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msg2dict(msg):\n",
    "    result = dict()\n",
    "    if 'note_on' in msg:\n",
    "        on_ = True\n",
    "    elif 'note_off' in msg:\n",
    "        on_ = False\n",
    "    else:\n",
    "        on_ = None\n",
    "    result['time'] = int(msg[msg.rfind('time'):].split(' ')[0].split('=')[1].translate(\n",
    "        str.maketrans({a: None for a in string.punctuation})))\n",
    "\n",
    "    if on_ is not None:\n",
    "        for k in ['note', 'velocity']:\n",
    "            result[k] = int(msg[msg.rfind(k):].split(' ')[0].split('=')[1].translate(\n",
    "                str.maketrans({a: None for a in string.punctuation})))\n",
    "    return [result, on_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c3e745-94ea-4532-abaa-17d056b84025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_note(last_state, note, velocity, on_=True):\n",
    "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of this range will be ignored\n",
    "    result = [0] * 88 if last_state is None else last_state.copy()\n",
    "    if 21 <= note <= 108:\n",
    "        result[note-21] = velocity if on_ else 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da72959b-b59d-4382-8ee0-d4cd087aa34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state(new_msg, last_state):\n",
    "    new_msg, on_ = msg2dict(str(new_msg))\n",
    "    new_state = switch_note(last_state, note=new_msg['note'], velocity=new_msg['velocity'], on_=on_) if on_ is not None else last_state\n",
    "    return [new_state, new_msg['time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b369205-7228-4d85-8084-c590ec6df99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track2seq(track):\n",
    "    result = []\n",
    "    last_state, last_time = get_new_state(str(track[0]), [0]*88)\n",
    "    for i in range(1, len(track)):\n",
    "        new_state, new_time = get_new_state(track[i], last_state)\n",
    "        if new_time > 0:\n",
    "            result += [last_state]*new_time\n",
    "        last_state, last_time = new_state, new_time\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501e86aa-36fe-4d0b-8e73-da14641f8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid2arry(mid, min_msg_pct=0.1):\n",
    "    tracks_len = [len(tr) for tr in mid.tracks]\n",
    "    min_n_msg = max(tracks_len) * min_msg_pct\n",
    "    # convert each track to nested list\n",
    "    all_arys = []\n",
    "    for i in range(len(mid.tracks)):\n",
    "        if len(mid.tracks[i]) > min_n_msg:\n",
    "            ary_i = track2seq(mid.tracks[i])\n",
    "            all_arys.append(ary_i)\n",
    "    # make all nested list the same length\n",
    "    max_len = max([len(ary) for ary in all_arys])\n",
    "    for i in range(len(all_arys)):\n",
    "        if len(all_arys[i]) < max_len:\n",
    "            all_arys[i] += [[0] * 88] * (max_len - len(all_arys[i]))\n",
    "    all_arys = np.array(all_arys)\n",
    "    all_arys = all_arys.max(axis=0)\n",
    "    # trim: remove consecutive 0s in the beginning and at the end\n",
    "    sums = all_arys.sum(axis=1)\n",
    "    ends = np.where(sums > 0)[0]\n",
    "    return all_arys[min(ends): max(ends)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "102b475c-6469-40ed-902d-1c46b343e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38db7098-2f92-4516-8fd8-a605562c2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generator(model_path, optimizer_path, lr, latent_dim):\n",
    "    model = Generator(latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path))\n",
    "    return model, optimizer\n",
    "\n",
    "def load_discriminator(model_path, optimizer_path, lr):\n",
    "    model = Discriminator().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path))\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53226d6-5201-4ac4-be0f-05e397778700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_optimizer_generator(model, optimizer, epoch, model_dir=\"model_checkpoints_2\"):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, f\"generator_epoch_{epoch+1}.pth\")\n",
    "    optimizer_path = os.path.join(model_dir, f\"generator_optimizer_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "    print(f\"Generator model and optimizer states saved after epoch {epoch+1}\")\n",
    "\n",
    "def save_model_and_optimizer_discriminator(model, optimizer, epoch, model_dir=\"model_checkpoints_2\"):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, f\"discriminator_epoch_{epoch+1}.pth\")\n",
    "    optimizer_path = os.path.join(model_dir, f\"discriminator_optimizer_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "    print(f\"Discriminator model and optimizer states saved after epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f57e9d-33a7-4197-a525-bcc502c354f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDIDataset(Dataset):\n",
    "    def __init__(self, midi_dir, max_files=30000):\n",
    "        self.file_paths = [os.path.join(midi_dir, f) for f in os.listdir(midi_dir) if f.endswith('.mid')]\n",
    "        random.shuffle(self.file_paths)\n",
    "        self.file_paths = self.file_paths[:max_files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for _ in range(10):  # Try up to 10 times before giving up\n",
    "            path = self.file_paths[idx]\n",
    "            try:\n",
    "                mid = mido.MidiFile(path)\n",
    "                arr = mid2arry(mid)\n",
    "\n",
    "                if arr is None or len(arr) < 4000:\n",
    "                    #print(f\"Skipping (too short or silent): {path}\")\n",
    "                    idx = (idx + 1) % len(self.file_paths)\n",
    "                    continue\n",
    "\n",
    "                arr = arr[:8000]  # Cap at 8000 rows\n",
    "                arr = np.array(arr, dtype=np.float32)\n",
    "                arr /= 127.0\n",
    "                tensor = torch.from_numpy(arr).clone()\n",
    "                del arr\n",
    "                gc.collect()\n",
    "                return tensor\n",
    "\n",
    "            except Exception as e:\n",
    "                #print(f\"Failed at {path}: {e}\")\n",
    "                idx = (idx + 1) % len(self.file_paths)\n",
    "\n",
    "        # If it fails 10 times in a row, just return a zero tensor\n",
    "        print(\"Too many failures. Returning dummy tensor.\")\n",
    "        return torch.zeros((8000, 88), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f5f2aa4-372f-404e-ac03-6a5b37875173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads each sample in the batch to exactly 8000 rows (sequence length).\n",
    "    \"\"\"\n",
    "    target_len = 8000\n",
    "    processed_batch = []\n",
    "\n",
    "    for x in batch:\n",
    "        length = x.size(0)\n",
    "        if length < target_len:\n",
    "            pad_len = target_len - length\n",
    "            padding = torch.zeros((pad_len, x.size(1)))\n",
    "            x_padded = torch.cat([x, padding], dim=0)\n",
    "        else:\n",
    "            x_padded = x[:target_len]\n",
    "        processed_batch.append(x_padded)\n",
    "\n",
    "    return torch.stack(processed_batch)  # Shape: (batch_size, 8000, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "230db264-7cc2-4c64-8b80-c5b8b23bb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MIDIDataset(midi_dir=\"/home/sakshisahemail/Desktop/GANs/Dataset/Full_Dataset/\", max_files=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d2857af-e3a0-45d4-9d49-56c3809c83f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MIDI files found: 30000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total MIDI files found: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b5928b6-0f4f-4104-aa37-c1b814a7badf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f117ff33-3fe8-4899-b1dd-8938f7b06986",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=midi_collate_fn,\n",
    "    num_workers=num_workers,  \n",
    "    pin_memory=True,    \n",
    "    drop_last=True      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ee8aff-7e6d-4965-bc73-5b378fe332fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Starting from latent vector\n",
    "            nn.Linear(latent_dim, 256 * 500 * 11),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (256, 500, 11)),  # Shape: (256, 500, 11)\n",
    "\n",
    "            # Upsample to (128, 1000, 22)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Upsample to (64, 2000, 22) — only height increases\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=(4, 3), stride=(2, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Upsample to (32, 4000, 44)\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Final upsample to (1, 8000, 88)\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92657892-01f4-44d0-a5af-0b55b9de5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=4),      # [B, 64, 2000, 22]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=4),    # [B, 128, 500, 5]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # [B, 256, ~250, ~3]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),  # [B, 512, ~125, ~2]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        # Flatten size calculation\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 8000, 88)\n",
    "            out = self.features(dummy_input)\n",
    "            flattened_size = out.view(1, -1).size(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7ccf5fd-03d6-443b-90d6-b705dceabf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_D = 0.0004  # slow it down\n",
    "lr_G = 0.0008  # let G catch up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e1e31e5-e40a-4adb-a741-4e0d971d2453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50043/503539930.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "/tmp/ipykernel_50043/503539930.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(optimizer_path))\n",
      "/tmp/ipykernel_50043/503539930.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "/tmp/ipykernel_50043/503539930.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(optimizer_path))\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100  \n",
    "\n",
    "G, optimizer_G = load_generator(\n",
    "    \"model_checkpoints_1/generator_epoch_10.pth\",\n",
    "    \"model_checkpoints_1/generator_optimizer_epoch_10.pth\",\n",
    "    lr_G,\n",
    "    latent_dim\n",
    ")\n",
    "\n",
    "D, optimizer_D = load_discriminator(\n",
    "    \"model_checkpoints_1/discriminator_epoch_10.pth\",\n",
    "    \"model_checkpoints_1/discriminator_optimizer_epoch_10.pth\",\n",
    "    lr_D\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41b33a6c-3431-4970-9cd2-cbfd0dc2fdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=128000, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.to(device)\n",
    "D.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d817203-3f4b-4782-a746-97d4668e0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "latent_dim = 100\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bb64859-5af3-4f69-b9f9-a7ab52deca3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/10 | Avg Loss D: 0.4395 | Avg Loss G: 11.1276\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2/10 | Avg Loss D: 0.3919 | Avg Loss G: 10.6032\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3/10 | Avg Loss D: 0.3975 | Avg Loss G: 9.0208\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4/10 | Avg Loss D: 0.3595 | Avg Loss G: 9.7055\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5/10 | Avg Loss D: 0.3746 | Avg Loss G: 9.9467\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6/10 | Avg Loss D: 0.4592 | Avg Loss G: 10.5539\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7/10 | Avg Loss D: 0.3853 | Avg Loss G: 7.8659\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8/10 | Avg Loss D: 0.3529 | Avg Loss G: 10.1053\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9/10 | Avg Loss D: 0.3467 | Avg Loss G: 11.2942\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10/10 | Avg Loss D: 0.3523 | Avg Loss G: 13.0075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "G.train()\n",
    "D.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"\\nEpoch {epoch+1}/{10}\")\n",
    "    \n",
    "    epoch_loss_D = 0.0\n",
    "    epoch_loss_G = 0.0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "\n",
    "    for batch_idx, real_imgs in progress_bar:\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        real_imgs = real_imgs.unsqueeze(1)\n",
    "        b_size = real_imgs.size(0)\n",
    "\n",
    "        # === Train Discriminator ===\n",
    "        z = torch.randn(b_size, latent_dim, device=device)\n",
    "        fake_imgs = G(z)\n",
    "\n",
    "        D_real = D(real_imgs).view(-1)\n",
    "        D_fake = D(fake_imgs.detach()).view(-1)\n",
    "\n",
    "        real_labels = torch.full_like(D_real, 0.9, device=device)  # label smoothing\n",
    "        fake_labels = torch.zeros_like(D_fake, device=device)\n",
    "\n",
    "        loss_D_real = criterion(D_real, real_labels)\n",
    "        loss_D_fake = criterion(D_fake, fake_labels)\n",
    "        loss_D = loss_D_real + loss_D_fake\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # === Train Generator ===\n",
    "        D_fake = D(fake_imgs).view(-1)\n",
    "        real_gen_labels = torch.ones_like(D_fake, device=device)\n",
    "        loss_G = criterion(D_fake, real_gen_labels)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        epoch_loss_D += loss_D.item()\n",
    "        epoch_loss_G += loss_G.item()\n",
    "\n",
    "        # Update tqdm bar with loss info\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss D': f\"{loss_D.item():.4f}\",\n",
    "            'Loss G': f\"{loss_G.item():.4f}\"\n",
    "        })\n",
    "\n",
    "    avg_loss_D = epoch_loss_D / len(dataloader)\n",
    "    avg_loss_G = epoch_loss_G / len(dataloader)\n",
    "    print(f\"✅ Epoch {epoch+1}/{10} | Avg Loss D: {avg_loss_D:.4f} | Avg Loss G: {avg_loss_G:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00536176-30f6-4366-b2c9-803095174a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator model and optimizer states saved after epoch 10\n",
      "Discriminator model and optimizer states saved after epoch 10\n"
     ]
    }
   ],
   "source": [
    "save_model_and_optimizer_generator(G, optimizer_G, epoch)\n",
    "save_model_and_optimizer_discriminator(D, optimizer_D, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18809a29-f698-4d81-8661-ea81a23da959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_samples(generator, num_samples=500, output_dir=\"generated_samples_round_2\", batch_size=1):\n",
    "    import gc\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    generator.eval()\n",
    "\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "    sample_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(num_batches):\n",
    "            current_batch_size = min(batch_size, num_samples - sample_count)\n",
    "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
    "            fake_imgs = generator(z)\n",
    "\n",
    "            fake_imgs = fake_imgs.squeeze(1).cpu().numpy()\n",
    "            for i, img in enumerate(fake_imgs):\n",
    "                np.save(os.path.join(output_dir, f\"generated_sample_{sample_count + 1}.npy\"), img)\n",
    "                sample_count += 1\n",
    "\n",
    "            # Memory cleanup\n",
    "            del z, fake_imgs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"Generated {num_samples} samples and saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fd3dea9-8227-49a0-a594-858b919a376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 500 samples and saved to generated_samples_round_2\n"
     ]
    }
   ],
   "source": [
    "generate_and_save_samples(G, num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a7c92-0647-4e90-9b5b-46179b3603f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.load('path_to_file.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
